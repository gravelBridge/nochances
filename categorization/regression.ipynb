{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Yourui/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "import json\n",
    "import torch\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "colleges_list = open('all-colleges.txt').readlines()\n",
    "colleges_list = [college[:college.index(' (')] for college in colleges_list]\n",
    "\n",
    "try:\n",
    "    data = json.load(open('standardized_output.json', 'r'))\n",
    "except:\n",
    "    data = json.load(open('output_2.json'))\n",
    "    for post in data.values():\n",
    "        drop = []\n",
    "        for i in range(len(post['results'])):\n",
    "            college = post['results'][i]\n",
    "            closest_name = difflib.get_close_matches(college['school_name'], colleges_list, n=1, cutoff=0.8)\n",
    "            if closest_name:\n",
    "                college['school_name'] = closest_name[0]\n",
    "            else:\n",
    "                drop.append(i)\n",
    "        for index in reversed(drop):\n",
    "            post['results'].pop(index)\n",
    "\n",
    "    json.dump(data, open('standardized_output.json', 'w'))\n",
    "\n",
    "shortened_data = {}\n",
    "\n",
    "for post_id in data.keys():\n",
    "    if post_id >= '189wc0k':\n",
    "        shortened_data.update({post_id: data[post_id]})\n",
    "\n",
    "data = shortened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yourui/Documents/nochances/nochances/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Size: 11040\n"
     ]
    }
   ],
   "source": [
    "from regressor import ResultRegressor, CollegeResultsDataset\n",
    "\n",
    "split_index = int(0.8 * len(data))\n",
    "train_data = dict(list(data.items())[:split_index])\n",
    "test_data = dict(list(data.items())[split_index:])\n",
    "train_data_size = sum(len(post['results']) for post in train_data.values())\n",
    "print(f\"Train Data Size: {train_data_size}\")\n",
    "\n",
    "train_dataset = CollegeResultsDataset(train_data, colleges_list, nltk_stopwords)\n",
    "test_dataset = CollegeResultsDataset(test_data, colleges_list, nltk_stopwords)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, n_epochs, device):\n",
    "    for epoch in range(n_epochs):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Current learning rate: {param_group['lr']}\")\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(device)\n",
    "\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, batch['target'])\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"Batch: {i}, Loss: {round(loss.item(), 2)}\")\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_total += batch['target'].size(0)\n",
    "            train_correct += ((outputs > 0.5) == batch['target']).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                for k, v in batch.items():\n",
    "                    batch[k] = v.to(device)\n",
    "\n",
    "                outputs = model(batch)\n",
    "                loss = criterion(outputs, batch['target'])\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_total += batch['target'].size(0)\n",
    "                val_correct += ((outputs > 0.5) == batch['target']).sum().item()\n",
    "\n",
    "                print(outputs)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        torch.save(model.state_dict(), '2023_24_regression.pth')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yourui/Documents/nochances/categorization/regressor.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.model(**inputs)['last_hidden_state'][0][0], dtype=torch.float32)\n",
      "/Users/Yourui/Documents/nochances/categorization/regressor.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.model(**inputs)['last_hidden_state'][0][0], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 0, Loss: 0.66\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 1, Loss: 0.74\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 2, Loss: 0.64\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 3, Loss: 0.78\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 4, Loss: 0.74\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 5, Loss: 0.8\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 6, Loss: 0.58\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 7, Loss: 0.58\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 8, Loss: 0.78\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 9, Loss: 0.65\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 10, Loss: 0.65\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 11, Loss: 0.57\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 12, Loss: 0.65\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 13, Loss: 0.39\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 14, Loss: 0.8\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 15, Loss: 0.63\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 16, Loss: 0.75\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 17, Loss: 0.56\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 18, Loss: 0.82\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 19, Loss: 1.0\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 20, Loss: 0.43\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 21, Loss: 0.63\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 22, Loss: 0.5\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 23, Loss: 0.59\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 24, Loss: 0.54\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 25, Loss: 0.63\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 26, Loss: 0.78\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 27, Loss: 0.53\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 28, Loss: 0.61\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 29, Loss: 0.83\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 30, Loss: 0.62\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 31, Loss: 0.68\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 32, Loss: 0.92\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 33, Loss: 0.89\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 34, Loss: 0.79\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 35, Loss: 0.64\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 36, Loss: 0.77\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 37, Loss: 0.79\n",
      "torch.Size([10, 50]) torch.Size([10, 50]) torch.Size([10, 300]) torch.Size([10, 500])\n",
      "Batch: 38, Loss: 0.68\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m      9\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, n_epochs, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/Documents/nochances/nochances/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/nochances/nochances/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/nochances/nochances/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/nochances/nochances/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/connection.py:1136\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ResultRegressor().to(device)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000000, gamma=1.0)\n",
    "\n",
    "n_epochs = 50\n",
    "model = train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, n_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '2023_24_regression.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nochances",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
