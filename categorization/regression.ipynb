{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yourui/Documents/nochances/nochances/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer, DistilBertConfig\n",
    "from torch.utils.data import DataLoader\n",
    "import nltk\n",
    "import torch\n",
    "import difflib\n",
    "import json\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Yourui/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "nltk_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "colleges_list = open('../categorization/all-colleges.txt').readlines()\n",
    "colleges_list = [college[:college.index(' (')] for college in colleges_list]\n",
    "\n",
    "try:\n",
    "    data = json.load(open('../categorization/standardized_output.json', 'r'))\n",
    "except:\n",
    "    data = json.load(open('../categorization/output_2.json'))\n",
    "\n",
    "    for post in data.values():\n",
    "        drop = []\n",
    "        for i in range(len(post['results'])):\n",
    "            college = post['results'][i]\n",
    "            closest_name = difflib.get_close_matches(college['school_name'], colleges_list, n=1, cutoff=0.8)\n",
    "            if closest_name:\n",
    "                college['school_name'] = closest_name[0]\n",
    "            else:\n",
    "                drop.append(i)\n",
    "        for index in reversed(drop):\n",
    "            post['results'].pop(index)\n",
    "\n",
    "    json.dump(data, open('../categorization/standardized_output.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_data = {}\n",
    "\n",
    "for post_id in data.keys():\n",
    "    if post_id >= '189wc0k':\n",
    "        shortened_data.update({post_id: data[post_id]})\n",
    "\n",
    "data = shortened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultRegressor(torch.nn.Module):\n",
    "    def __init__(self, stopwords):\n",
    "        super(ResultRegressor, self).__init__()\n",
    "        self.stopwords = stopwords\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, padding=True)\n",
    "        \n",
    "        self.text1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\",\n",
    "                                                     ignore_mismatched_sizes=True,\n",
    "                                                     config = DistilBertConfig(max_position_embeddings=20, \n",
    "                                                                               dropout=0.5))\n",
    "        self.text2 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                                     ignore_mismatched_sizes=True,\n",
    "                                                     config = DistilBertConfig(max_position_embeddings=10, \n",
    "                                                                               dropout=0.5))\n",
    "        self.text3 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                                     ignore_mismatched_sizes=True,\n",
    "                                                     config = DistilBertConfig(max_position_embeddings=512, \n",
    "                                                                               dropout=0.5))\n",
    "        \n",
    "        self.pc1 = torch.nn.Linear(768, 128)\n",
    "        self.pc2 = torch.nn.Linear(768, 128)\n",
    "        self.pc3 = torch.nn.Linear(768, 128)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(394, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 1)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "    def ts(self, input):\n",
    "        return torch.tensor(input, dtype=torch.long).to(device)\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        filtered_text = [w for w in text.split() if w.lower() not in self.stopwords]\n",
    "        return \" \".join(filtered_text)\n",
    "\n",
    "    def tokenize(self, input, max_length=512):\n",
    "        tokenized = self.tokenizer.encode_plus(\n",
    "            input,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        return torch.tensor(tokenized['input_ids'], dtype=torch.long), torch.tensor(tokenized['attention_mask'], dtype=torch.long)\n",
    "\n",
    "    def forward(self, x, results, college_id):\n",
    "        numerical_inputs = self.ts([int(x['ethnicity']),\n",
    "                            int(x['gender']),\n",
    "                            int(x['income_bracket']),\n",
    "                            int(x['gpa']),\n",
    "                            int(x['apib_number']),\n",
    "                            int(x['apib_scores']),\n",
    "                            int(x['standardized_test_scores']),\n",
    "                            int(results['in_state']),\n",
    "                            int(results['round']),\n",
    "                            college_id])\n",
    "        \n",
    "        major_ids, major_masks = self.tokenize(self.remove_stopwords(x['major']), max_length=20)\n",
    "        residence_ids, residence_masks = self.tokenize(self.remove_stopwords(x['residence']), max_length=10)\n",
    "        extracurricular_ids, extracurricular_masks =  self.tokenize(self.remove_stopwords('\\n'.join(x['extracurriculars'] + x['awards'])))\n",
    "\n",
    "        major_pooler = self.text1(self.ts(major_ids), self.ts(major_masks))[0][:,0]\n",
    "        residence_pooler = self.text2(self.ts(residence_ids), self.ts(residence_masks))[0][:,0]\n",
    "        extraccurricular_pooler = self.text3(self.ts(extracurricular_ids), self.ts(extracurricular_masks))[0][:,0]\n",
    "        \n",
    "        numerical_inputs = torch.cat([numerical_inputs,\n",
    "                                      self.relu(self.pc1(major_pooler))[0],\n",
    "                                      self.relu(self.pc2(residence_pooler))[0],\n",
    "                                      self.relu(self.pc3(extraccurricular_pooler))[0]])\n",
    "\n",
    "\n",
    "        x = self.fc1(numerical_inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.softmax(x)[0].type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized because the shapes did not match:\n",
      "- distilbert.embeddings.position_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([20, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized because the shapes did not match:\n",
      "- distilbert.embeddings.position_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ResultRegressor(stopwords=nltk_stopwords).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "total_training_start_time = time.time()\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = []\n",
    "\n",
    "# for post in data.values():\n",
    "#     for college in post['results']:\n",
    "#         inputs = [int(post['ethnicity']),\n",
    "#                   int(post['gender']),\n",
    "#                   int(post['income_bracket']),\n",
    "#                   int(post['gpa']),\n",
    "#                   int(post['apib_number']),\n",
    "#                   int(post['apib_scores']),\n",
    "#                   int(post['standardized_test_scores']),\n",
    "#                   int(college['in_state']),\n",
    "#                   int(college['round']),\n",
    "#                   colleges_list.index(college['school_name']),\n",
    "#                   post['major'],\n",
    "#                   post['residence'],\n",
    "#                   '\\n'.join(post['extracurriculars'] + post['awards'])]\n",
    "#         y = college['accepted']\n",
    "\n",
    "#         dataset.append((inputs, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(0.8 * len(data))\n",
    "\n",
    "train_data = dict(list(data.items())[:split_index])\n",
    "test_data = dict(list(data.items())[split_index:])\n",
    "\n",
    "total_count = sum(len(post['results']) for post in train_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11040"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yourui/Documents/nochances/nochances/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/var/folders/h1/88vswkjs65x3v7m1ytlwgpy40000gp/T/ipykernel_9768/3387472898.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(input, dtype=torch.long).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 1.0, Target: 1.0, Loss: -0.0, Data: 1/11040\n",
      "Predicted: 1.0, Target: 1.0, Loss: -0.0, Data: 2/11040\n",
      "Predicted: 1.0, Target: 1.0, Loss: -0.0, Data: 3/11040\n",
      "Predicted: 1.0, Target: 1.0, Loss: -0.0, Data: 4/11040\n",
      "Predicted: 1.0, Target: 1.0, Loss: -0.0, Data: 5/11040\n",
      "Predicted: 1.0, Target: 0.0, Loss: 100.0, Data: 6/11040\n",
      "Predicted: 1.0, Target: 0.0, Loss: 100.0, Data: 7/11040\n",
      "Predicted: 1.0, Target: 1.0, Loss: -0.0, Data: 8/11040\n",
      "Predicted: 1.0, Target: 1.0, Loss: -0.0, Data: 9/11040\n",
      "Predicted: 1.0, Target: 1.0, Loss: -0.0, Data: 10/11040\n",
      "Predicted: 1.0, Target: 1.0, Loss: -0.0, Data: 11/11040\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    for post in train_data.values():\n",
    "        for college in post['results']:\n",
    "            i += 1\n",
    "\n",
    "            college_id = colleges_list.index(college['school_name'])\n",
    "            predicted = model(post, college, college_id)\n",
    "            target = torch.tensor(college['accepted'], dtype=torch.float32).to(device)\n",
    "\n",
    "            loss = criterion(predicted, target)\n",
    "            loss.backward()\n",
    "\n",
    "            print(f'Predicted: {predicted}, Target: {target}, Loss: {loss.item()}, Data: {i}/{total_count}')\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            total += 1\n",
    "            correct += 1 if predicted == target else 0\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time = epoch_end_time - epoch_start_time\n",
    "    print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {running_loss/len(train_data):.4f}, Accuracy: {correct/total:.4f}, Time: {epoch_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'categorization/2023_24_regression.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nochances",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
