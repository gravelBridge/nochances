{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yourui/Documents/nochances/nochances/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from numerical_regressor import TokenNumericCollegeResultsDataset, CombinedDelayedRegressor, mappings\n",
    "\n",
    "combined_data = {}\n",
    "uncategorized_data = json.load(open('../../scraping/combined_collegeresults_data.json', 'r'))\n",
    "standardized_output = json.load(open('../../categorization/standardized_output.json', 'r'))\n",
    "\n",
    "colleges_list = open('../../categorization/all-colleges.txt').readlines()\n",
    "colleges_list = [college[:college.index(' (')] for college in colleges_list]\n",
    "\n",
    "college_data = pd.read_csv('../../categorization/college_acceptance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_data = pd.read_csv('../../categorization/major_data.csv')\n",
    "major_data['combined'] = major_data[major_data.columns.drop(['Name', 'Total'])].values.tolist()\n",
    "major_data = major_data[['Name', 'combined', 'Total']]\n",
    "def to_frequencies(counts, total):\n",
    "    return [float(count/total) if total else 0 for count in counts]\n",
    "major_data['combined'] = major_data.apply(lambda x: to_frequencies(x['combined'], x['Total']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>combined</th>\n",
       "      <th>Total</th>\n",
       "      <th>Applicants total</th>\n",
       "      <th>Admissions total</th>\n",
       "      <th>Enrolled total</th>\n",
       "      <th>Percent of freshmen submitting SAT scores</th>\n",
       "      <th>Percent of freshmen submitting ACT scores</th>\n",
       "      <th>SAT Critical Reading 25th percentile score</th>\n",
       "      <th>SAT Critical Reading 75th percentile score</th>\n",
       "      <th>...</th>\n",
       "      <th>Percent of freshmen  receiving federal grant aid</th>\n",
       "      <th>Percent of freshmen receiving Pell grants</th>\n",
       "      <th>Percent of freshmen receiving other federal grant aid</th>\n",
       "      <th>Percent of freshmen receiving state/local grant aid</th>\n",
       "      <th>Percent of freshmen receiving institutional grant aid</th>\n",
       "      <th>Percent of freshmen receiving student loan aid</th>\n",
       "      <th>Percent of freshmen receiving federal student loans</th>\n",
       "      <th>Percent of freshmen receiving other loan aid</th>\n",
       "      <th>Endowment assets (year end) per FTE enrollment (GASB)</th>\n",
       "      <th>Endowment assets (year end) per FTE enrollment (FASB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama A &amp; M University</td>\n",
       "      <td>[0.1782178217821782, 0.11287128712871287, 0.23...</td>\n",
       "      <td>505</td>\n",
       "      <td>6142.0</td>\n",
       "      <td>5521.0</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>450.0</td>\n",
       "      <td>...</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>University of Alabama at Birmingham</td>\n",
       "      <td>[0.10211565585331453, 0.051057827926657265, 0....</td>\n",
       "      <td>3545</td>\n",
       "      <td>5689.0</td>\n",
       "      <td>4934.0</td>\n",
       "      <td>1773.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>520.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24136.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>University of Alabama in Huntsville</td>\n",
       "      <td>[0.3168114460909555, 0.11088400613183444, 0.24...</td>\n",
       "      <td>1957</td>\n",
       "      <td>2054.0</td>\n",
       "      <td>1656.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11502.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama State University</td>\n",
       "      <td>[0.01282051282051282, 0.12307692307692308, 0.2...</td>\n",
       "      <td>390</td>\n",
       "      <td>10245.0</td>\n",
       "      <td>5251.0</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13202.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The University of Alabama</td>\n",
       "      <td>[0.13406237343053867, 0.015525853921965708, 0....</td>\n",
       "      <td>7407</td>\n",
       "      <td>30975.0</td>\n",
       "      <td>17515.0</td>\n",
       "      <td>6454.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>490.0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>19469.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>Providence Christian College</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>21</td>\n",
       "      <td>122.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>Polytechnic University of Puerto Rico-Orlando</td>\n",
       "      <td>[0.64, 0.04, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>University of North Georgia</td>\n",
       "      <td>[0.0031409501374165686, 0.06949352179034157, 0...</td>\n",
       "      <td>2547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>Kennesaw State University</td>\n",
       "      <td>[0.1752136752136752, 0.14213358657803102, 0.22...</td>\n",
       "      <td>6318</td>\n",
       "      <td>9471.0</td>\n",
       "      <td>5355.0</td>\n",
       "      <td>3194.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>590.0</td>\n",
       "      <td>...</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>Husson University</td>\n",
       "      <td>[0.0, 0.010714285714285714, 0.5642857142857143...</td>\n",
       "      <td>560</td>\n",
       "      <td>1581.0</td>\n",
       "      <td>1263.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>530.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3673.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1266 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Name  \\\n",
       "0                          Alabama A & M University   \n",
       "1               University of Alabama at Birmingham   \n",
       "2               University of Alabama in Huntsville   \n",
       "3                          Alabama State University   \n",
       "4                         The University of Alabama   \n",
       "...                                             ...   \n",
       "1261                   Providence Christian College   \n",
       "1262  Polytechnic University of Puerto Rico-Orlando   \n",
       "1263                    University of North Georgia   \n",
       "1264                      Kennesaw State University   \n",
       "1265                              Husson University   \n",
       "\n",
       "                                               combined  Total  \\\n",
       "0     [0.1782178217821782, 0.11287128712871287, 0.23...    505   \n",
       "1     [0.10211565585331453, 0.051057827926657265, 0....   3545   \n",
       "2     [0.3168114460909555, 0.11088400613183444, 0.24...   1957   \n",
       "3     [0.01282051282051282, 0.12307692307692308, 0.2...    390   \n",
       "4     [0.13406237343053867, 0.015525853921965708, 0....   7407   \n",
       "...                                                 ...    ...   \n",
       "1261  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     21   \n",
       "1262  [0.64, 0.04, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0....     25   \n",
       "1263  [0.0031409501374165686, 0.06949352179034157, 0...   2547   \n",
       "1264  [0.1752136752136752, 0.14213358657803102, 0.22...   6318   \n",
       "1265  [0.0, 0.010714285714285714, 0.5642857142857143...    560   \n",
       "\n",
       "      Applicants total  Admissions total  Enrolled total  \\\n",
       "0               6142.0            5521.0          1104.0   \n",
       "1               5689.0            4934.0          1773.0   \n",
       "2               2054.0            1656.0           651.0   \n",
       "3              10245.0            5251.0          1479.0   \n",
       "4              30975.0           17515.0          6454.0   \n",
       "...                ...               ...             ...   \n",
       "1261             122.0              65.0            20.0   \n",
       "1262               NaN               NaN             NaN   \n",
       "1263               NaN               NaN             NaN   \n",
       "1264            9471.0            5355.0          3194.0   \n",
       "1265            1581.0            1263.0           444.0   \n",
       "\n",
       "      Percent of freshmen submitting SAT scores  \\\n",
       "0                                          15.0   \n",
       "1                                           6.0   \n",
       "2                                          34.0   \n",
       "3                                          18.0   \n",
       "4                                          23.0   \n",
       "...                                         ...   \n",
       "1261                                        NaN   \n",
       "1262                                        NaN   \n",
       "1263                                        NaN   \n",
       "1264                                       67.0   \n",
       "1265                                       99.0   \n",
       "\n",
       "      Percent of freshmen submitting ACT scores  \\\n",
       "0                                          88.0   \n",
       "1                                          93.0   \n",
       "2                                          94.0   \n",
       "3                                          87.0   \n",
       "4                                          76.0   \n",
       "...                                         ...   \n",
       "1261                                        NaN   \n",
       "1262                                        NaN   \n",
       "1263                                        NaN   \n",
       "1264                                       45.0   \n",
       "1265                                        5.0   \n",
       "\n",
       "      SAT Critical Reading 25th percentile score  \\\n",
       "0                                          370.0   \n",
       "1                                          520.0   \n",
       "2                                          510.0   \n",
       "3                                          380.0   \n",
       "4                                          490.0   \n",
       "...                                          ...   \n",
       "1261                                         NaN   \n",
       "1262                                         NaN   \n",
       "1263                                         NaN   \n",
       "1264                                       500.0   \n",
       "1265                                       420.0   \n",
       "\n",
       "      SAT Critical Reading 75th percentile score  ...  \\\n",
       "0                                          450.0  ...   \n",
       "1                                          640.0  ...   \n",
       "2                                          640.0  ...   \n",
       "3                                          480.0  ...   \n",
       "4                                          620.0  ...   \n",
       "...                                          ...  ...   \n",
       "1261                                         NaN  ...   \n",
       "1262                                         NaN  ...   \n",
       "1263                                         NaN  ...   \n",
       "1264                                       590.0  ...   \n",
       "1265                                       530.0  ...   \n",
       "\n",
       "      Percent of freshmen  receiving federal grant aid  \\\n",
       "0                                                 81.0   \n",
       "1                                                 36.0   \n",
       "2                                                 31.0   \n",
       "3                                                 76.0   \n",
       "4                                                 20.0   \n",
       "...                                                ...   \n",
       "1261                                              50.0   \n",
       "1262                                             100.0   \n",
       "1263                                               NaN   \n",
       "1264                                              36.0   \n",
       "1265                                              29.0   \n",
       "\n",
       "      Percent of freshmen receiving Pell grants  \\\n",
       "0                                          81.0   \n",
       "1                                          36.0   \n",
       "2                                          31.0   \n",
       "3                                          76.0   \n",
       "4                                          18.0   \n",
       "...                                         ...   \n",
       "1261                                       50.0   \n",
       "1262                                      100.0   \n",
       "1263                                        NaN   \n",
       "1264                                       36.0   \n",
       "1265                                       29.0   \n",
       "\n",
       "      Percent of freshmen receiving other federal grant aid  \\\n",
       "0                                                   7.0       \n",
       "1                                                  10.0       \n",
       "2                                                   4.0       \n",
       "3                                                  13.0       \n",
       "4                                                   4.0       \n",
       "...                                                 ...       \n",
       "1261                                               14.0       \n",
       "1262                                                0.0       \n",
       "1263                                                NaN       \n",
       "1264                                                2.0       \n",
       "1265                                               26.0       \n",
       "\n",
       "      Percent of freshmen receiving state/local grant aid  \\\n",
       "0                                                   1.0     \n",
       "1                                                   0.0     \n",
       "2                                                   1.0     \n",
       "3                                                  11.0     \n",
       "4                                                   3.0     \n",
       "...                                                 ...     \n",
       "1261                                                0.0     \n",
       "1262                                               50.0     \n",
       "1263                                                NaN     \n",
       "1264                                               68.0     \n",
       "1265                                               23.0     \n",
       "\n",
       "      Percent of freshmen receiving institutional grant aid  \\\n",
       "0                                                  32.0       \n",
       "1                                                  60.0       \n",
       "2                                                  63.0       \n",
       "3                                                  34.0       \n",
       "4                                                  50.0       \n",
       "...                                                 ...       \n",
       "1261                                              100.0       \n",
       "1262                                                0.0       \n",
       "1263                                                NaN       \n",
       "1264                                                4.0       \n",
       "1265                                               61.0       \n",
       "\n",
       "      Percent of freshmen receiving student loan aid  \\\n",
       "0                                               89.0   \n",
       "1                                               56.0   \n",
       "2                                               46.0   \n",
       "3                                               81.0   \n",
       "4                                               42.0   \n",
       "...                                              ...   \n",
       "1261                                            64.0   \n",
       "1262                                            50.0   \n",
       "1263                                             NaN   \n",
       "1264                                            51.0   \n",
       "1265                                            61.0   \n",
       "\n",
       "      Percent of freshmen receiving federal student loans  \\\n",
       "0                                                  89.0     \n",
       "1                                                  55.0     \n",
       "2                                                  46.0     \n",
       "3                                                  81.0     \n",
       "4                                                  41.0     \n",
       "...                                                 ...     \n",
       "1261                                               64.0     \n",
       "1262                                               50.0     \n",
       "1263                                                NaN     \n",
       "1264                                               51.0     \n",
       "1265                                               60.0     \n",
       "\n",
       "      Percent of freshmen receiving other loan aid  \\\n",
       "0                                              1.0   \n",
       "1                                              5.0   \n",
       "2                                              3.0   \n",
       "3                                              0.0   \n",
       "4                                              8.0   \n",
       "...                                            ...   \n",
       "1261                                          14.0   \n",
       "1262                                           0.0   \n",
       "1263                                           NaN   \n",
       "1264                                           3.0   \n",
       "1265                                          17.0   \n",
       "\n",
       "      Endowment assets (year end) per FTE enrollment (GASB)  \\\n",
       "0                                                   NaN       \n",
       "1                                               24136.0       \n",
       "2                                               11502.0       \n",
       "3                                               13202.0       \n",
       "4                                               19469.0       \n",
       "...                                                 ...       \n",
       "1261                                                NaN       \n",
       "1262                                                NaN       \n",
       "1263                                                NaN       \n",
       "1264                                             1180.0       \n",
       "1265                                                NaN       \n",
       "\n",
       "      Endowment assets (year end) per FTE enrollment (FASB)  \n",
       "0                                                   NaN      \n",
       "1                                                   NaN      \n",
       "2                                                   NaN      \n",
       "3                                                   NaN      \n",
       "4                                                   NaN      \n",
       "...                                                 ...      \n",
       "1261                                              350.0      \n",
       "1262                                                NaN      \n",
       "1263                                                NaN      \n",
       "1264                                                NaN      \n",
       "1265                                             3673.0      \n",
       "\n",
       "[1266 rows x 110 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "college_data = pd.merge(major_data, college_data, on=\"Name\")\n",
    "college_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oakwood University',\n",
       " 'Spring Hill College',\n",
       " 'Stillman College',\n",
       " 'Tuskegee University',\n",
       " 'Alaska Pacific University',\n",
       " 'University of Arkansas at Pine Bluff',\n",
       " 'California Institute of Technology',\n",
       " 'California Lutheran University',\n",
       " 'California Polytechnic State University-San Luis Obispo',\n",
       " 'California State University-Fullerton',\n",
       " 'California State University-Long Beach',\n",
       " 'California State University-Northridge',\n",
       " 'University of California-Berkeley',\n",
       " 'University of California-Davis',\n",
       " 'University of California-Irvine',\n",
       " 'University of California-Los Angeles',\n",
       " 'University of California-San Diego',\n",
       " 'University of California-Santa Barbara',\n",
       " 'Chapman University',\n",
       " 'San Diego Christian College',\n",
       " 'Claremont McKenna College',\n",
       " 'Harvey Mudd College',\n",
       " 'University of La Verne',\n",
       " 'La Sierra University',\n",
       " 'Menlo College',\n",
       " 'Occidental College',\n",
       " 'Hope International University',\n",
       " 'Pepperdine University',\n",
       " 'Pitzer College',\n",
       " 'Pomona College',\n",
       " 'San Diego State University',\n",
       " 'University of San Diego',\n",
       " 'Scripps College',\n",
       " 'University of Southern California',\n",
       " 'Colorado College',\n",
       " 'Colorado School of Mines',\n",
       " 'Connecticut College',\n",
       " 'Trinity College',\n",
       " 'Wesleyan University',\n",
       " 'Yale University',\n",
       " 'Delaware State University',\n",
       " 'American University',\n",
       " 'George Washington University',\n",
       " 'Georgetown University',\n",
       " 'Howard University',\n",
       " 'Barry University',\n",
       " 'University of Central Florida',\n",
       " 'Florida Agricultural and Mechanical University',\n",
       " 'Florida Atlantic University',\n",
       " 'Florida International University',\n",
       " 'Florida Memorial University',\n",
       " 'Florida Southern College',\n",
       " 'University of Florida',\n",
       " 'Jacksonville University',\n",
       " 'University of Miami',\n",
       " 'Warner University',\n",
       " 'The University of West Florida',\n",
       " 'Albany State University',\n",
       " 'Clayton  State University',\n",
       " 'Emmanuel College',\n",
       " 'Emory University',\n",
       " 'Fort Valley State University',\n",
       " 'Paine College',\n",
       " 'Spelman College',\n",
       " 'Toccoa Falls College',\n",
       " 'Wesleyan College',\n",
       " 'Chicago State University',\n",
       " 'University of Chicago',\n",
       " 'Northwestern University',\n",
       " 'Rockford University',\n",
       " 'Calumet College of Saint Joseph',\n",
       " 'University of Notre Dame',\n",
       " 'Grinnell College',\n",
       " 'Bethany College',\n",
       " 'Newman University',\n",
       " 'University of Saint Mary',\n",
       " 'Sterling College',\n",
       " 'Alice Lloyd College',\n",
       " 'Berea College',\n",
       " 'Brescia University',\n",
       " 'Kentucky State University',\n",
       " 'Northern Kentucky University',\n",
       " 'Union College',\n",
       " 'Dillard University',\n",
       " 'Grambling State University',\n",
       " 'Southeastern Louisiana University',\n",
       " 'Southern University and A & M College',\n",
       " 'Southern University at New Orleans',\n",
       " 'Tulane University of Louisiana',\n",
       " 'Bates College',\n",
       " 'Bowdoin College',\n",
       " 'Colby College',\n",
       " 'Washington Adventist University',\n",
       " 'Coppin State University',\n",
       " 'Johns Hopkins University',\n",
       " 'University of Maryland-College Park',\n",
       " 'Notre Dame of Maryland University',\n",
       " 'Amherst College',\n",
       " 'Bentley University',\n",
       " 'Boston College',\n",
       " 'Boston University',\n",
       " 'Brandeis University',\n",
       " 'Emerson College',\n",
       " 'Gordon College',\n",
       " 'Harvard University',\n",
       " 'College of the Holy Cross',\n",
       " 'Massachusetts Institute of Technology',\n",
       " 'Mount Holyoke College',\n",
       " 'Northeastern University',\n",
       " 'Smith College',\n",
       " 'Tufts University',\n",
       " 'Wellesley College',\n",
       " 'Williams College',\n",
       " 'Andrews University',\n",
       " 'Lawrence Technological University',\n",
       " 'University of Michigan-Ann Arbor',\n",
       " 'Carleton College',\n",
       " 'Macalester College',\n",
       " 'University of Minnesota-Twin Cities',\n",
       " 'Belhaven University',\n",
       " 'Millsaps College',\n",
       " 'Mississippi University for Women',\n",
       " 'Mississippi Valley State University',\n",
       " 'Rust College',\n",
       " 'Tougaloo College',\n",
       " 'William Carey University',\n",
       " 'Missouri Valley College',\n",
       " 'University of Missouri-Kansas City',\n",
       " 'College of the Ozarks',\n",
       " 'Washington University in St Louis',\n",
       " 'Dartmouth College',\n",
       " 'New Jersey City University',\n",
       " 'Princeton University',\n",
       " 'Stevens Institute of Technology',\n",
       " 'The College of New Jersey',\n",
       " 'New Mexico Institute of Mining and Technology',\n",
       " 'Bard College',\n",
       " 'Barnard College',\n",
       " 'Colgate University',\n",
       " 'Columbia University in the City of New York',\n",
       " 'Cornell University',\n",
       " 'CUNY Bernard M Baruch College',\n",
       " 'CUNY Brooklyn College',\n",
       " 'CUNY City College',\n",
       " 'CUNY Hunter College',\n",
       " 'CUNY John Jay College of Criminal Justice',\n",
       " 'CUNY Lehman College',\n",
       " 'CUNY Queens College',\n",
       " 'CUNY York College',\n",
       " 'Fashion Institute of Technology',\n",
       " 'Fordham University',\n",
       " 'Hamilton College',\n",
       " 'Hobart William Smith Colleges',\n",
       " 'Marist College',\n",
       " 'New York University',\n",
       " 'Rensselaer Polytechnic Institute',\n",
       " 'University of Rochester',\n",
       " 'St Lawrence University',\n",
       " 'Skidmore College',\n",
       " 'Stony Brook University',\n",
       " 'SUNY College at Geneseo',\n",
       " 'State University of New York at New Paltz',\n",
       " 'SUNY at Purchase College',\n",
       " 'SUNY College at Plattsburgh',\n",
       " 'Syracuse University',\n",
       " 'Vassar College',\n",
       " 'Barton College',\n",
       " 'Brevard College',\n",
       " 'Catawba College',\n",
       " 'Davidson College',\n",
       " 'Duke University',\n",
       " 'Gardner-Webb University',\n",
       " 'Johnson C Smith University',\n",
       " 'University of North Carolina at Chapel Hill',\n",
       " 'Mid-Atlantic Christian University',\n",
       " 'Wake Forest University',\n",
       " 'Western Carolina University',\n",
       " 'Dickinson State University',\n",
       " 'Case Western Reserve University',\n",
       " 'Central State University',\n",
       " 'Denison University',\n",
       " 'Kenyon College',\n",
       " 'Oberlin College',\n",
       " 'Ohio Dominican University',\n",
       " 'Tiffin University',\n",
       " 'Wilmington College',\n",
       " 'Oklahoma Wesleyan University',\n",
       " 'Oklahoma Panhandle State University',\n",
       " 'Oral Roberts University',\n",
       " 'University of Tulsa',\n",
       " 'Reed College',\n",
       " 'Corban University',\n",
       " 'Bryn Mawr College',\n",
       " 'Bucknell University',\n",
       " 'Carnegie Mellon University',\n",
       " 'Dickinson College',\n",
       " 'Franklin and Marshall College',\n",
       " 'Gettysburg College',\n",
       " 'Haverford College',\n",
       " 'Lafayette College',\n",
       " 'Lehigh University',\n",
       " 'Muhlenberg College',\n",
       " 'University of Pennsylvania',\n",
       " 'Swarthmore College',\n",
       " 'Villanova University',\n",
       " 'Washington & Jefferson College',\n",
       " 'Brown University',\n",
       " 'Lander University',\n",
       " 'Bryan College-Dayton',\n",
       " 'Christian Brothers University',\n",
       " 'Cumberland University',\n",
       " 'Fisk University',\n",
       " 'Lane College',\n",
       " 'Le Moyne-Owen College',\n",
       " 'Southern Adventist University',\n",
       " 'Vanderbilt University',\n",
       " 'Abilene Christian University',\n",
       " 'Dallas Baptist University',\n",
       " 'Hardin-Simmons University',\n",
       " 'Huston-Tillotson University',\n",
       " 'Texas A & M International University',\n",
       " 'LeTourneau University',\n",
       " 'Prairie View A & M University',\n",
       " 'Rice University',\n",
       " 'Southwestern Assemblies of God University',\n",
       " 'The University of Texas at Austin',\n",
       " 'Texas Christian University',\n",
       " 'Texas Southern University',\n",
       " 'Texas Wesleyan University',\n",
       " 'Brigham Young University-Hawaii',\n",
       " 'Middlebury College',\n",
       " 'Hampton University',\n",
       " 'Liberty University',\n",
       " 'University of Richmond',\n",
       " 'University of Virginia-Main Campus',\n",
       " 'Virginia Military Institute',\n",
       " 'Washington and Lee University',\n",
       " 'Alderson Broaddus University',\n",
       " 'Concord University',\n",
       " 'West Virginia State University',\n",
       " 'Stanford University',\n",
       " 'Soka University of America',\n",
       " 'California State University-Monterey Bay',\n",
       " 'Trine University-Regional/Non-Traditional Campuses']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residences = [value['residence'] for value in standardized_output.values()]\n",
    "all_numerical_data = [json.loads(line) for line in open('../../categorization/categorized.json', 'r').readlines()]\n",
    "selective_enough = college_data.loc[college_data['Admissions total']/college_data['Applicants total'] < .50]['Name'].to_list()\n",
    "selective_enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data_index = 0\n",
    "for i, numerical_data in enumerate(all_numerical_data):\n",
    "\n",
    "    if numerical_data == {'skip': True}:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        post_id = list(uncategorized_data.keys())[i]\n",
    "\n",
    "        selected_data = [\n",
    "            numerical_data['basic_info']['ethnicity'],\n",
    "            numerical_data['basic_info']['gender'],\n",
    "            numerical_data['basic_info']['income_bracket'],\n",
    "            numerical_data['basic_info']['gpa'],\n",
    "            numerical_data['basic_info']['ap_ib_courses'] ** (1/2),\n",
    "            numerical_data['basic_info']['ap_ib_scores'],\n",
    "            numerical_data['basic_info']['test_score'],\n",
    "            numerical_data['basic_info']['location'],\n",
    "            numerical_data['basic_info']['first_gen'],\n",
    "        ] + list(numerical_data['ecs'].values()) + list(numerical_data['awards'].values())\n",
    "\n",
    "        try:\n",
    "            other_data = standardized_output[post_id]\n",
    "            results = other_data['results']\n",
    "            combined_data.update({post_id:{\n",
    "                'major':other_data['major'],\n",
    "                'residence':other_data['residence'],\n",
    "                'ecs':other_data['extracurriculars'],\n",
    "                'awards':other_data['awards'],\n",
    "                'numeric':selected_data,\n",
    "                'results':results\n",
    "            }})\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    # Change this to randomized data, except that the gpa is shit (~2.5 and selectivity is <50%)\n",
    "    except IndexError:\n",
    "        \n",
    "        random_posts = []\n",
    "        for _ in range(5):\n",
    "            post = random.choice(all_numerical_data)\n",
    "            while post == {'skip': True}:\n",
    "                post = random.choice(all_numerical_data)\n",
    "            random_posts.append(post)\n",
    "\n",
    "        selected_data = [\n",
    "            random.choice([0, 1]), ## Race\n",
    "            random.choice([0, 1]), ## Gender\n",
    "            random.choice([0, 1, 2, 3, 4]), ## Income Bracket\n",
    "            random.choice([0, 1]), ## GPA\n",
    "            random_posts[0]['basic_info']['ap_ib_courses'] ** (1/2),\n",
    "            random_posts[1]['basic_info']['ap_ib_scores'],\n",
    "            random_posts[2]['basic_info']['test_score'],\n",
    "            random_posts[3]['basic_info']['location'],\n",
    "            random_posts[4]['basic_info']['first_gen'],\n",
    "        ] + list(numerical_data['ecs'].values()) + list(numerical_data['awards'].values())\n",
    "\n",
    "        combined_data.update({i: {\n",
    "            'major': mappings[random.randint(0,11)][0],\n",
    "            'residence': random.choice(residences),\n",
    "            'ecs': [],\n",
    "            'awards': [],\n",
    "            'numeric': selected_data,\n",
    "            'results': [{\n",
    "                'school_name': random.choice(selective_enough),\n",
    "                'in_state': 0,\n",
    "                'accepted': 0,\n",
    "                'round': random.choice([1, 3]),\n",
    "            } for _ in range(random.randint(1,5))]\n",
    "        }})\n",
    "        fake_data_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3081"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = TokenNumericCollegeResultsDataset(combined_data, college_data)\n",
    "# torch.save(dataset, 'shortened_numerical_fake_data.pt')\n",
    "dataset = torch.load('shortened_numerical_fake_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Size: 17565\n"
     ]
    }
   ],
   "source": [
    "full_data_size = len(dataset)\n",
    "train_size = int(full_data_size * 0.8)\n",
    "print(f\"Train Data Size: {train_size}\")\n",
    "\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(0)\n",
    "\n",
    "batch_size = 128\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, full_data_size - train_size], generator=gen)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, n_epochs, device):\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"Current learning rate: {param_group['lr']}\")\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(device)\n",
    "\n",
    "            outputs = model(batch['inputs'])\n",
    "            loss = criterion(outputs, batch['target'])\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_total += batch['target'].size(0)\n",
    "            train_correct += ((torch.sigmoid(outputs) > 0.5) == batch['target']).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                for k, v in batch.items():\n",
    "                    batch[k] = v.to(device)\n",
    "\n",
    "                outputs = model(batch['inputs'])\n",
    "                loss = criterion(outputs, batch['target'])\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_total += batch['target'].size(0)\n",
    "                val_correct += ((torch.sigmoid(outputs) > 0.5) == batch['target']).sum().item()\n",
    "\n",
    "            print(torch.sigmoid(outputs))\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'reduced_v5_fake_data_numerical.pt')\n",
    "            #v2 is 81% val acc\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate: 0.0001\n",
      "tensor([0.6465, 0.5544, 0.5914, 0.5384, 0.5403, 0.5722, 0.5562, 0.5582, 0.5630,\n",
      "        0.5590, 0.5445, 0.5456, 0.6387, 0.5712, 0.5401, 0.5655, 0.5606, 0.5594,\n",
      "        0.5506, 0.5442, 0.5712, 0.5376, 0.6107, 0.6090, 0.6114, 0.5412, 0.5452,\n",
      "        0.5491, 0.5400, 0.6305, 0.5603, 0.5534, 0.5405, 0.6164, 0.5546, 0.5390,\n",
      "        0.5473, 0.5641, 0.5595, 0.5717], device='mps:0')\n",
      "Epoch 1/40:\n",
      "Train Loss: 0.6802, Train Acc: 0.6049\n",
      "Val Loss: 0.6645, Val Acc: 0.6061\n",
      "Current learning rate: 9.993191928767829e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yourui/Documents/nochances/nochances/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8153, 0.6351, 0.5309, 0.5829, 0.5380, 0.6218, 0.5810, 0.7484, 0.7569,\n",
      "        0.5316, 0.8866, 0.5311, 0.5222, 0.5686, 0.5540, 0.8055, 0.5854, 0.7917,\n",
      "        0.5952, 0.5179, 0.5291, 0.6122, 0.8133, 0.5437, 0.5361, 0.5888, 0.5372,\n",
      "        0.5277, 0.5165, 0.8093, 0.5280, 0.6501, 0.6919, 0.6097, 0.7437, 0.7003,\n",
      "        0.7540, 0.5519, 0.5592, 0.6760], device='mps:0')\n",
      "Epoch 2/40:\n",
      "Train Loss: 0.6390, Train Acc: 0.6047\n",
      "Val Loss: 0.6082, Val Acc: 0.6061\n",
      "Current learning rate: 9.994296201012241e-05\n",
      "tensor([0.9130, 0.4033, 0.4778, 0.8766, 0.6789, 0.9255, 0.9039, 0.5853, 0.4305,\n",
      "        0.4966, 0.9134, 0.8119, 0.6078, 0.8985, 0.5129, 0.6430, 0.4790, 0.8977,\n",
      "        0.5971, 0.6829, 0.4762, 0.6819, 0.6503, 0.4865, 0.5968, 0.7041, 0.4948,\n",
      "        0.4709, 0.4656, 0.4372, 0.6150, 0.4887, 0.7035, 0.4168, 0.4535, 0.9118,\n",
      "        0.4261, 0.8722, 0.8834, 0.9116], device='mps:0')\n",
      "Epoch 3/40:\n",
      "Train Loss: 0.6136, Train Acc: 0.6432\n",
      "Val Loss: 0.5753, Val Acc: 0.6995\n",
      "Current learning rate: 9.994897729606563e-05\n",
      "tensor([0.3511, 0.4389, 0.5151, 0.5478, 0.9317, 0.5606, 0.4496, 0.9286, 0.3453,\n",
      "        0.5575, 0.9402, 0.8507, 0.4169, 0.4343, 0.3527, 0.6869, 0.7888, 0.4094,\n",
      "        0.5676, 0.5671, 0.8462, 0.4272, 0.5065, 0.4227, 0.6909, 0.7930, 0.4751,\n",
      "        0.8805, 0.5680, 0.6388, 0.3676, 0.3848, 0.8783, 0.4329, 0.5810, 0.7732,\n",
      "        0.6330, 0.6399, 0.4661, 0.8284], device='mps:0')\n",
      "Epoch 4/40:\n",
      "Train Loss: 0.5778, Train Acc: 0.7020\n",
      "Val Loss: 0.5578, Val Acc: 0.7197\n",
      "Current learning rate: 9.995203368256803e-05\n",
      "tensor([0.8871, 0.4300, 0.6266, 0.6124, 0.7465, 0.4502, 0.9536, 0.5764, 0.3817,\n",
      "        0.9082, 0.5531, 0.5780, 0.5661, 0.7527, 0.5470, 0.3170, 0.5313, 0.5019,\n",
      "        0.4102, 0.8454, 0.8951, 0.8431, 0.5094, 0.8600, 0.3667, 0.9453, 0.6110,\n",
      "        0.4078, 0.3615, 0.1915, 0.5624, 0.9322, 0.4259, 0.7423, 0.5349, 0.6664,\n",
      "        0.2860, 0.6137, 0.4604, 0.5049], device='mps:0')\n",
      "Epoch 5/40:\n",
      "Train Loss: 0.5695, Train Acc: 0.7205\n",
      "Val Loss: 0.5467, Val Acc: 0.7297\n",
      "Current learning rate: 9.995391565441954e-05\n",
      "tensor([0.3226, 0.8373, 0.6238, 0.5063, 0.9712, 0.7722, 0.7238, 0.3845, 0.7844,\n",
      "        0.4364, 0.5564, 0.4959, 0.8602, 0.4580, 0.3594, 0.6101, 0.9899, 0.3939,\n",
      "        0.6924, 0.8926, 0.5896, 0.9038, 0.4431, 0.4356, 0.4993, 0.8361, 0.6442,\n",
      "        0.5290, 0.7838, 0.5744, 0.2455, 0.6165, 0.8961, 0.5501, 0.7442, 0.4721,\n",
      "        0.4123, 0.1064, 0.6250, 0.5271], device='mps:0')\n",
      "Epoch 6/40:\n",
      "Train Loss: 0.5550, Train Acc: 0.7310\n",
      "Val Loss: 0.5356, Val Acc: 0.7327\n",
      "Current learning rate: 9.99557626033584e-05\n",
      "tensor([0.9630, 0.2221, 0.9659, 0.4661, 0.5339, 0.9403, 0.5457, 0.9378, 0.7667,\n",
      "        0.9214, 0.4103, 0.5320, 0.4295, 0.8415, 0.3757, 0.5555, 0.7512, 0.9240,\n",
      "        0.8081, 0.9210, 0.4614, 0.6275, 0.6099, 0.2931, 0.5606, 0.9294, 0.8159,\n",
      "        0.1800, 0.4547, 0.6474, 0.8182, 0.5268, 0.9697, 0.3678, 0.6337, 0.5715,\n",
      "        0.5948, 0.4657, 0.5862, 0.9441], device='mps:0')\n",
      "Epoch 7/40:\n",
      "Train Loss: 0.5423, Train Acc: 0.7392\n",
      "Val Loss: 0.5259, Val Acc: 0.7427\n",
      "Current learning rate: 9.995735301796319e-05\n",
      "tensor([0.2434, 0.4934, 0.7046, 0.5039, 0.3803, 0.5957, 0.4040, 0.8090, 0.5889,\n",
      "        0.5947, 0.5313, 0.5451, 0.9082, 0.3532, 0.6772, 0.3748, 0.5764, 0.2756,\n",
      "        0.8454, 0.8409, 0.5374, 0.5728, 0.2533, 0.6052, 0.5268, 0.2541, 0.6111,\n",
      "        0.4757, 0.3202, 0.7060, 0.5474, 0.9080, 0.1291, 0.6602, 0.3874, 0.3437,\n",
      "        0.1957, 0.5511, 0.5492, 0.7404], device='mps:0')\n",
      "Epoch 8/40:\n",
      "Train Loss: 0.5388, Train Acc: 0.7458\n",
      "Val Loss: 0.5129, Val Acc: 0.7518\n",
      "Current learning rate: 9.99594356262598e-05\n",
      "tensor([0.4319, 0.5722, 0.5262, 0.7412, 0.3046, 0.1345, 0.9659, 0.7670, 0.9110,\n",
      "        0.5122, 0.4758, 0.3503, 0.9526, 0.8354, 0.3061, 0.2520, 0.3124, 0.7583,\n",
      "        0.5641, 0.6219, 0.7351, 0.4077, 0.3601, 0.4620, 0.7174, 0.6491, 0.8882,\n",
      "        0.8695, 0.9374, 0.4305, 0.6892, 0.5837, 0.9447, 0.6670, 0.5151, 0.8283,\n",
      "        0.4466, 0.5050, 0.9895, 0.9319], device='mps:0')\n",
      "Epoch 9/40:\n",
      "Train Loss: 0.5259, Train Acc: 0.7546\n",
      "Val Loss: 0.4989, Val Acc: 0.7582\n",
      "Current learning rate: 9.996162037359768e-05\n",
      "tensor([0.6258, 0.4676, 0.3113, 0.5111, 0.4641, 0.7634, 0.8184, 0.7181, 0.7495,\n",
      "        0.1125, 0.9738, 0.5535, 0.2276, 0.4104, 0.2665, 0.5926, 0.7711, 0.5912,\n",
      "        0.2282, 0.3398, 0.9315, 0.5168, 0.3006, 0.5811, 0.1758, 0.4796, 0.4995,\n",
      "        0.9738, 0.9166, 0.8808, 0.5530, 0.4858, 0.8136, 0.9206, 0.4601, 0.7833,\n",
      "        0.9829, 0.7339, 0.5504, 0.9602], device='mps:0')\n",
      "Epoch 10/40:\n",
      "Train Loss: 0.5180, Train Acc: 0.7612\n",
      "Val Loss: 0.4901, Val Acc: 0.7634\n",
      "Current learning rate: 9.99629703042971e-05\n",
      "tensor([0.7489, 0.9059, 0.9273, 0.9397, 0.5177, 0.6439, 0.9456, 0.6703, 0.3786,\n",
      "        0.6412, 0.7198, 0.7836, 0.2266, 0.6298, 0.9838, 0.6717, 0.1673, 0.7038,\n",
      "        0.6374, 0.2233, 0.1261, 0.9671, 0.8663, 0.1523, 0.5247, 0.9364, 0.6501,\n",
      "        0.9355, 0.9309, 0.8687, 0.9789, 0.5983, 0.1533, 0.8105, 0.5634, 0.5174,\n",
      "        0.9667, 0.4178, 0.5102, 0.7852], device='mps:0')\n",
      "Epoch 11/40:\n",
      "Train Loss: 0.5083, Train Acc: 0.7632\n",
      "Val Loss: 0.4835, Val Acc: 0.7680\n",
      "Current learning rate: 9.996395294645024e-05\n",
      "tensor([0.1622, 0.5254, 0.9804, 0.6953, 0.2200, 0.5887, 0.5207, 0.7005, 0.8311,\n",
      "        0.3956, 0.5694, 0.1981, 0.4641, 0.8879, 0.5397, 0.4747, 0.8428, 0.8533,\n",
      "        0.9443, 0.6036, 0.1354, 0.6087, 0.7211, 0.4942, 0.3227, 0.8691, 0.8467,\n",
      "        0.9403, 0.7346, 0.7680, 0.7969, 0.3677, 0.9051, 0.5917, 0.5413, 0.6282,\n",
      "        0.6963, 0.4580, 0.9943, 0.3116], device='mps:0')\n",
      "Epoch 12/40:\n",
      "Train Loss: 0.4984, Train Acc: 0.7704\n",
      "Val Loss: 0.4814, Val Acc: 0.7691\n",
      "Current learning rate: 9.996426651676823e-05\n",
      "tensor([0.5706, 0.4068, 0.2187, 0.0704, 0.8128, 0.4428, 0.7176, 0.5091, 0.8112,\n",
      "        0.6407, 0.6499, 0.9705, 0.4307, 0.3442, 0.1223, 0.8937, 0.6943, 0.8465,\n",
      "        0.5077, 0.9034, 0.6388, 0.8199, 0.6401, 0.6178, 0.8641, 0.3601, 0.9382,\n",
      "        0.2564, 0.5554, 0.9526, 0.3021, 0.5235, 0.4755, 0.8599, 0.1490, 0.5092,\n",
      "        0.4467, 0.9723, 0.4499, 0.1886], device='mps:0')\n",
      "Epoch 13/40:\n",
      "Train Loss: 0.4929, Train Acc: 0.7753\n",
      "Val Loss: 0.4749, Val Acc: 0.7771\n",
      "Current learning rate: 9.996522383882086e-05\n",
      "tensor([0.1707, 0.3714, 0.9149, 0.5874, 0.9681, 0.4344, 0.6660, 0.9985, 0.7605,\n",
      "        0.1986, 0.6355, 0.6144, 0.9200, 0.9448, 0.4202, 0.4362, 0.9160, 0.1898,\n",
      "        0.6583, 0.4130, 0.2189, 0.8988, 0.8331, 0.7138, 0.6549, 0.5976, 0.9046,\n",
      "        0.3895, 0.2099, 0.3906, 0.9306, 0.4443, 0.0442, 0.8494, 0.4114, 0.7427,\n",
      "        0.4311, 0.9995, 0.9519, 0.9933], device='mps:0')\n",
      "Epoch 14/40:\n",
      "Train Loss: 0.4897, Train Acc: 0.7785\n",
      "Val Loss: 0.4654, Val Acc: 0.7814\n",
      "Current learning rate: 9.996660328458067e-05\n",
      "tensor([0.5232, 0.6751, 0.9224, 0.9728, 0.6955, 0.4667, 0.3755, 0.7310, 0.9237,\n",
      "        0.8148, 0.4444, 0.9270, 0.9725, 0.1888, 0.6675, 0.5244, 0.2687, 0.1616,\n",
      "        0.7449, 0.9060, 0.2128, 0.4398, 0.6208, 0.6433, 0.6177, 0.8920, 0.7953,\n",
      "        0.9927, 0.5736, 0.4298, 0.9235, 0.7212, 0.5735, 0.2687, 0.7203, 0.2390,\n",
      "        0.9237, 0.0480, 0.5213, 0.4868], device='mps:0')\n",
      "Epoch 15/40:\n",
      "Train Loss: 0.4802, Train Acc: 0.7831\n",
      "Val Loss: 0.4610, Val Acc: 0.7880\n",
      "Current learning rate: 9.996722700711256e-05\n",
      "tensor([0.9827, 0.6882, 0.9977, 0.6118, 0.8427, 0.4560, 0.5487, 0.4445, 0.4335,\n",
      "        0.7136, 0.7639, 0.6191, 0.2981, 0.7473, 0.9817, 0.3143, 0.2059, 0.4822,\n",
      "        0.7583, 0.3998, 0.2761, 0.3352, 0.5923, 0.2886, 0.3889, 0.5778, 0.9695,\n",
      "        0.3715, 0.7693, 0.6926, 0.5773, 0.5440, 0.5264, 0.4731, 0.8398, 0.7973,\n",
      "        0.2283, 0.9189, 0.6897, 0.9518], device='mps:0')\n",
      "Epoch 16/40:\n",
      "Train Loss: 0.4725, Train Acc: 0.7839\n",
      "Val Loss: 0.4575, Val Acc: 0.7869\n",
      "Current learning rate: 9.996772345488924e-05\n",
      "tensor([0.5917, 0.8771, 0.9679, 0.4921, 0.7220, 0.9747, 0.1848, 0.6599, 0.2037,\n",
      "        0.9304, 0.7609, 0.6909, 0.6426, 0.7365, 0.3716, 0.1586, 0.7594, 0.9981,\n",
      "        0.9552, 0.5879, 0.9989, 0.5181, 0.1128, 0.7246, 0.2845, 0.6809, 0.6241,\n",
      "        0.5871, 0.9648, 0.1819, 0.6197, 0.8209, 0.1471, 0.5579, 0.7473, 0.4387,\n",
      "        0.9971, 0.6849, 0.6124, 0.5850], device='mps:0')\n",
      "Epoch 17/40:\n",
      "Train Loss: 0.4719, Train Acc: 0.7839\n",
      "Val Loss: 0.4543, Val Acc: 0.7867\n",
      "Current learning rate: 9.996817829484218e-05\n",
      "tensor([0.9742, 0.6363, 0.3271, 0.5586, 0.6535, 0.4128, 0.6607, 0.2450, 0.7085,\n",
      "        0.8007, 0.2463, 0.9984, 0.9884, 0.6775, 0.0951, 0.6130, 0.6581, 0.4815,\n",
      "        0.9914, 0.5698, 0.9662, 0.6246, 0.9079, 0.5231, 0.6679, 0.1812, 0.0671,\n",
      "        0.5280, 0.1385, 0.8987, 0.4535, 0.9381, 0.6115, 0.8446, 0.9290, 0.4326,\n",
      "        0.9643, 0.0612, 0.2746, 0.1431], device='mps:0')\n",
      "Epoch 18/40:\n",
      "Train Loss: 0.4622, Train Acc: 0.7949\n",
      "Val Loss: 0.4505, Val Acc: 0.7901\n",
      "Current learning rate: 9.99687038351928e-05\n",
      "tensor([0.9414, 0.4599, 0.9620, 0.2919, 0.4660, 0.1141, 0.9932, 0.8861, 0.5340,\n",
      "        0.5452, 0.9421, 0.4731, 0.5009, 0.7625, 0.6518, 0.9922, 0.1936, 0.9705,\n",
      "        0.1048, 0.8346, 0.3191, 0.5338, 0.8338, 0.9177, 0.4940, 0.9928, 0.0990,\n",
      "        0.9442, 0.6622, 0.6409, 0.9995, 0.5421, 0.9439, 0.5841, 0.1993, 0.6627,\n",
      "        0.8067, 0.9798, 1.0000, 0.5446], device='mps:0')\n",
      "Epoch 19/40:\n",
      "Train Loss: 0.4681, Train Acc: 0.7917\n",
      "Val Loss: 0.4482, Val Acc: 0.7844\n",
      "Current learning rate: 9.99690242694291e-05\n",
      "tensor([0.3013, 0.8970, 0.2928, 0.9922, 0.5529, 0.1099, 0.1001, 0.0481, 0.6369,\n",
      "        0.8366, 0.9789, 0.7071, 0.4314, 0.2956, 0.3559, 0.6546, 0.4825, 0.5417,\n",
      "        0.6444, 0.6746, 0.8809, 0.9139, 0.0946, 0.9138, 0.9842, 0.2987, 0.3556,\n",
      "        0.3385, 0.5345, 0.1226, 0.7383, 0.8415, 0.4791, 0.5610, 0.0872, 0.2243,\n",
      "        0.5352, 0.2778, 0.8510, 0.8852], device='mps:0')\n",
      "Epoch 20/40:\n",
      "Train Loss: 0.4569, Train Acc: 0.7961\n",
      "Val Loss: 0.4473, Val Acc: 0.7978\n",
      "Current learning rate: 9.996914603785607e-05\n",
      "tensor([0.2454, 0.8629, 0.3255, 0.9842, 0.6307, 0.4145, 0.7025, 0.2823, 0.4389,\n",
      "        0.5902, 0.5634, 0.5396, 0.6500, 0.4923, 0.0361, 0.4759, 0.3255, 0.3957,\n",
      "        0.5740, 0.9047, 0.6904, 0.5194, 0.6045, 0.4888, 0.9757, 0.7704, 0.9339,\n",
      "        0.7485, 0.8106, 0.3028, 0.9181, 0.3644, 0.3113, 0.4362, 0.9615, 0.5093,\n",
      "        0.0357, 0.0613, 0.4929, 0.2097], device='mps:0')\n",
      "Epoch 21/40:\n",
      "Train Loss: 0.4507, Train Acc: 0.7981\n",
      "Val Loss: 0.4485, Val Acc: 0.7958\n",
      "Current learning rate: 9.99689836927811e-05\n",
      "tensor([0.9508, 0.3152, 0.3208, 0.8076, 0.9448, 0.3892, 0.4053, 0.2922, 0.7782,\n",
      "        0.9758, 0.8633, 0.6270, 0.1941, 0.8472, 0.8232, 0.8969, 0.2068, 0.9969,\n",
      "        0.9797, 0.7012, 0.5678, 0.4696, 0.8452, 0.5891, 0.4329, 0.9508, 0.5689,\n",
      "        0.9277, 0.8604, 0.9847, 0.1264, 0.6088, 0.7757, 0.9677, 0.7125, 0.2846,\n",
      "        0.9331, 0.8103, 0.4187, 0.7792], device='mps:0')\n",
      "Epoch 22/40:\n",
      "Train Loss: 0.4518, Train Acc: 0.7974\n",
      "Val Loss: 0.4502, Val Acc: 0.7960\n",
      "Current learning rate: 9.99687468831797e-05\n",
      "tensor([0.8797, 0.7816, 0.7730, 0.5811, 0.1468, 0.8925, 0.3606, 0.9863, 0.3658,\n",
      "        0.3575, 0.3942, 0.3612, 0.3323, 0.2540, 0.9058, 0.3632, 0.7306, 0.6866,\n",
      "        0.9997, 0.9808, 0.4833, 0.9943, 0.5649, 0.5572, 0.9392, 0.4392, 0.9137,\n",
      "        0.7079, 0.6645, 0.8973, 0.7272, 0.6350, 0.3956, 0.3785, 0.8713, 0.4958,\n",
      "        0.3043, 0.8328, 0.6337, 0.8258], device='mps:0')\n",
      "Epoch 23/40:\n",
      "Train Loss: 0.4417, Train Acc: 0.8002\n",
      "Val Loss: 0.4456, Val Acc: 0.7976\n",
      "Current learning rate: 9.996937973188873e-05\n",
      "tensor([0.9408, 0.9825, 0.2062, 0.9819, 0.5326, 0.5195, 0.4417, 0.7031, 0.8276,\n",
      "        0.3185, 0.4500, 0.8260, 0.9949, 0.8858, 0.8907, 0.5094, 0.9724, 0.9977,\n",
      "        0.6647, 0.3103, 0.9520, 0.2571, 0.1552, 0.2039, 0.9759, 0.6624, 0.8307,\n",
      "        0.2511, 0.9962, 0.6448, 0.1697, 0.0920, 0.3430, 0.0443, 0.0989, 0.9015,\n",
      "        0.4479, 0.0566, 0.8709, 0.1087], device='mps:0')\n",
      "Epoch 24/40:\n",
      "Train Loss: 0.4412, Train Acc: 0.7989\n",
      "Val Loss: 0.4415, Val Acc: 0.7990\n",
      "Current learning rate: 9.996995022923215e-05\n",
      "tensor([0.8921, 0.8498, 0.1748, 0.3474, 0.4563, 0.4899, 0.9977, 0.8535, 0.8491,\n",
      "        0.0374, 0.5307, 0.6580, 0.9714, 0.6110, 0.8384, 0.5717, 0.5658, 0.8085,\n",
      "        0.7526, 0.6424, 0.3601, 0.9825, 0.8773, 0.0601, 0.8882, 0.5248, 0.7542,\n",
      "        0.0324, 0.7343, 0.2726, 0.8678, 0.2066, 0.0655, 0.9861, 0.0447, 0.2971,\n",
      "        0.1948, 0.9474, 0.6276, 0.3967], device='mps:0')\n",
      "Epoch 25/40:\n",
      "Train Loss: 0.4470, Train Acc: 0.8040\n",
      "Val Loss: 0.4427, Val Acc: 0.7960\n",
      "Current learning rate: 9.996977980075822e-05\n",
      "tensor([0.7977, 0.8732, 0.9662, 0.4738, 0.2542, 0.7228, 0.4672, 0.2749, 0.9989,\n",
      "        0.3201, 0.9997, 0.3235, 0.0887, 0.3115, 0.5872, 0.5480, 0.9678, 0.5873,\n",
      "        0.0843, 0.9912, 0.1975, 0.2598, 0.7529, 0.1620, 0.5555, 0.7721, 0.9744,\n",
      "        0.9805, 0.6221, 0.8060, 0.3083, 0.7525, 1.0000, 0.5282, 0.1339, 0.1636,\n",
      "        0.3251, 0.0768, 0.8268, 0.8196], device='mps:0')\n",
      "Epoch 26/40:\n",
      "Train Loss: 0.4355, Train Acc: 0.8087\n",
      "Val Loss: 0.4371, Val Acc: 0.8017\n",
      "Current learning rate: 9.997054489298908e-05\n",
      "tensor([0.3733, 0.3696, 0.9856, 0.8949, 0.1204, 0.3028, 0.0888, 0.8930, 0.2792,\n",
      "        0.6726, 0.8401, 0.9375, 0.3183, 0.6850, 0.4753, 0.8386, 0.5470, 0.7294,\n",
      "        0.6911, 0.7317, 0.9904, 0.2487, 0.7986, 0.1758, 0.9858, 0.9774, 0.7341,\n",
      "        0.7988, 0.5899, 0.2360, 0.7641, 0.5520, 0.8667, 0.2196, 0.4549, 0.9967,\n",
      "        0.1367, 0.9131, 0.0757, 0.8320], device='mps:0')\n",
      "Epoch 27/40:\n",
      "Train Loss: 0.4325, Train Acc: 0.8097\n",
      "Val Loss: 0.4384, Val Acc: 0.7974\n",
      "Current learning rate: 9.99703692224021e-05\n",
      "tensor([0.5860, 0.9530, 0.9641, 0.9935, 0.9116, 0.5783, 0.3692, 0.4428, 0.1642,\n",
      "        0.4441, 0.7393, 0.6672, 0.0970, 0.2652, 0.3637, 0.9796, 0.4831, 0.6921,\n",
      "        0.0425, 0.6183, 0.2180, 0.4999, 0.5164, 0.9873, 0.2282, 0.9989, 0.5594,\n",
      "        0.8801, 0.0813, 0.8293, 0.7533, 0.9994, 0.1120, 0.6044, 0.5295, 0.4083,\n",
      "        0.0943, 0.5591, 0.9652, 0.6907], device='mps:0')\n",
      "Epoch 28/40:\n",
      "Train Loss: 0.4307, Train Acc: 0.8071\n",
      "Val Loss: 0.4336, Val Acc: 0.8008\n",
      "Current learning rate: 9.997101329005312e-05\n",
      "tensor([0.0819, 0.7378, 0.0958, 0.0199, 0.1530, 0.0925, 0.3752, 0.1650, 0.9551,\n",
      "        0.2731, 0.8187, 0.4725, 0.9177, 0.4852, 0.8234, 0.6639, 0.1328, 0.8853,\n",
      "        0.9974, 0.2311, 0.2128, 0.5008, 0.3949, 0.9824, 0.9665, 0.6528, 0.8713,\n",
      "        0.1836, 0.4657, 0.3331, 0.8383, 0.8534, 0.1464, 0.9907, 0.5223, 0.9712,\n",
      "        0.5338, 0.2325, 0.0144, 0.1775], device='mps:0')\n",
      "Epoch 29/40:\n",
      "Train Loss: 0.4285, Train Acc: 0.8100\n",
      "Val Loss: 0.4282, Val Acc: 0.7983\n",
      "Current learning rate: 9.997172854265419e-05\n",
      "tensor([0.9609, 0.7730, 0.6611, 0.8369, 0.0694, 0.9813, 0.6467, 0.7240, 0.8642,\n",
      "        0.7027, 0.0815, 0.0694, 0.4534, 0.9489, 0.9855, 0.1327, 0.9855, 0.5250,\n",
      "        0.8779, 0.6846, 0.8667, 0.4409, 0.8971, 0.4395, 0.9075, 0.1303, 0.7235,\n",
      "        0.0915, 0.7842, 0.5468, 0.5858, 1.0000, 0.8608, 0.0319, 0.9528, 0.7229,\n",
      "        0.2248, 0.0927, 0.8127, 0.6540], device='mps:0')\n",
      "Epoch 30/40:\n",
      "Train Loss: 0.4248, Train Acc: 0.8138\n",
      "Val Loss: 0.4341, Val Acc: 0.8005\n",
      "Current learning rate: 9.997094327058864e-05\n",
      "tensor([0.3100, 0.8907, 0.7749, 0.5570, 0.7774, 0.6979, 0.9419, 0.2325, 0.6483,\n",
      "        0.0443, 0.7457, 0.4656, 0.9968, 0.3041, 0.5690, 0.8581, 0.9891, 0.8402,\n",
      "        0.9366, 0.6666, 0.8487, 0.5456, 0.4554, 0.0612, 0.9222, 0.6485, 0.5735,\n",
      "        0.1997, 0.4365, 0.6312, 0.1554, 0.3695, 0.9975, 0.8834, 0.8493, 0.6656,\n",
      "        0.9533, 0.5669, 0.1662, 0.9169], device='mps:0')\n",
      "Epoch 31/40:\n",
      "Train Loss: 0.4260, Train Acc: 0.8102\n",
      "Val Loss: 0.4309, Val Acc: 0.8005\n",
      "Current learning rate: 9.997136419667712e-05\n",
      "tensor([0.0799, 0.9219, 0.9965, 0.9824, 0.2708, 0.5109, 0.5822, 0.7746, 0.9356,\n",
      "        0.0629, 0.9084, 0.9699, 0.4124, 0.7453, 0.9981, 0.9320, 0.5164, 0.7489,\n",
      "        0.2293, 0.7809, 0.3145, 0.6251, 0.7934, 0.9818, 0.3985, 0.3152, 0.0791,\n",
      "        0.0961, 0.7932, 0.9937, 0.5574, 0.9876, 0.4147, 0.8212, 0.8653, 0.8128,\n",
      "        0.1925, 0.6184, 0.0152, 0.9534], device='mps:0')\n",
      "Epoch 32/40:\n",
      "Train Loss: 0.4205, Train Acc: 0.8106\n",
      "Val Loss: 0.4281, Val Acc: 0.8051\n",
      "Current learning rate: 9.99717369028276e-05\n",
      "tensor([9.9925e-01, 8.1204e-01, 2.5510e-01, 6.8764e-01, 9.9636e-01, 7.9250e-01,\n",
      "        8.6928e-01, 1.9852e-01, 8.0899e-02, 1.8587e-01, 2.4051e-01, 2.6092e-02,\n",
      "        7.5171e-01, 8.8599e-01, 8.8119e-01, 8.4126e-01, 6.3432e-01, 5.9715e-01,\n",
      "        8.8168e-01, 6.9271e-01, 9.9989e-01, 9.9242e-01, 4.8364e-01, 9.9945e-01,\n",
      "        9.8410e-02, 3.2945e-01, 4.4702e-01, 6.9486e-01, 8.4828e-01, 7.3105e-01,\n",
      "        3.3866e-02, 4.9283e-01, 7.6631e-01, 9.9062e-01, 9.7364e-01, 8.6333e-01,\n",
      "        1.4971e-01, 6.6322e-04, 9.5203e-01, 9.8456e-01], device='mps:0')\n",
      "Epoch 33/40:\n",
      "Train Loss: 0.4122, Train Acc: 0.8167\n",
      "Val Loss: 0.4290, Val Acc: 0.8012\n",
      "Current learning rate: 9.997162742056649e-05\n",
      "tensor([0.4857, 0.2854, 0.5937, 0.1734, 0.6662, 0.2995, 0.2011, 0.5077, 1.0000,\n",
      "        0.1832, 0.9621, 0.8196, 0.8056, 0.6277, 0.5692, 0.7639, 0.6944, 0.5469,\n",
      "        0.0648, 0.4043, 0.8726, 0.4276, 0.7367, 0.8789, 0.1183, 0.0284, 0.0301,\n",
      "        0.5364, 0.9659, 0.9918, 0.9788, 0.5530, 0.1967, 0.9345, 0.6536, 0.9764,\n",
      "        0.4713, 0.7554, 0.2587, 0.7261], device='mps:0')\n",
      "Epoch 34/40:\n",
      "Train Loss: 0.4145, Train Acc: 0.8129\n",
      "Val Loss: 0.4326, Val Acc: 0.8033\n",
      "Current learning rate: 9.997114389231959e-05\n",
      "tensor([0.2144, 0.9730, 0.7621, 0.7749, 0.9999, 0.1909, 0.9391, 0.8814, 0.7520,\n",
      "        0.2868, 0.1846, 0.5461, 0.9989, 0.9662, 0.9788, 0.8179, 0.8436, 0.1213,\n",
      "        0.9161, 0.9985, 1.0000, 0.1481, 1.0000, 0.5617, 0.3626, 0.7557, 0.9028,\n",
      "        0.9852, 0.9953, 0.5494, 0.9218, 0.8204, 0.4996, 0.6392, 0.4508, 0.2580,\n",
      "        0.8702, 0.2010, 0.6277, 0.5145], device='mps:0')\n",
      "Epoch 35/40:\n",
      "Train Loss: 0.4167, Train Acc: 0.8149\n",
      "Val Loss: 0.4305, Val Acc: 0.8019\n",
      "Current learning rate: 9.99714224880907e-05\n",
      "tensor([0.3258, 0.7125, 0.3185, 0.0040, 0.3331, 0.1371, 0.0203, 0.5869, 0.8627,\n",
      "        0.4869, 0.2618, 0.0416, 0.9870, 0.6058, 0.9966, 0.2293, 0.8243, 0.7809,\n",
      "        0.2059, 0.3700, 0.2324, 0.6846, 0.4248, 0.5320, 0.1053, 0.9929, 0.4596,\n",
      "        0.9998, 0.2732, 0.0948, 0.4826, 0.9991, 0.6531, 0.2730, 0.6098, 0.6837,\n",
      "        0.1328, 0.9123, 0.2193, 0.4721], device='mps:0')\n",
      "Epoch 36/40:\n",
      "Train Loss: 0.4111, Train Acc: 0.8187\n",
      "Val Loss: 0.4292, Val Acc: 0.8060\n",
      "Current learning rate: 9.9971593854693e-05\n",
      "tensor([0.7102, 0.9864, 0.5425, 0.9723, 0.9990, 0.9649, 0.4083, 0.8288, 0.4133,\n",
      "        0.4216, 0.2004, 0.3619, 0.1010, 0.9999, 0.4568, 0.9993, 0.7904, 0.8039,\n",
      "        1.0000, 0.9997, 0.3984, 0.1846, 0.2761, 0.3086, 0.6743, 0.4465, 0.6495,\n",
      "        0.7849, 0.1155, 0.9998, 0.9819, 0.7007, 0.0264, 0.5865, 0.8629, 0.4000,\n",
      "        0.9780, 0.0347, 0.2401, 0.9497], device='mps:0')\n",
      "Epoch 37/40:\n",
      "Train Loss: 0.4105, Train Acc: 0.8170\n",
      "Val Loss: 0.4283, Val Acc: 0.8040\n",
      "Current learning rate: 9.997171578033813e-05\n",
      "tensor([0.6425, 0.1032, 0.9090, 0.2425, 0.6576, 1.0000, 0.3312, 0.2077, 0.8678,\n",
      "        0.9415, 0.7911, 0.7168, 0.3456, 0.6297, 0.9980, 0.9591, 0.4661, 0.9463,\n",
      "        0.0060, 0.2751, 0.9566, 0.9449, 0.9958, 0.9482, 0.0182, 0.9955, 0.6505,\n",
      "        0.9223, 0.9999, 0.4388, 0.3501, 0.9993, 0.9825, 0.9853, 0.0203, 0.5611,\n",
      "        0.8960, 0.2729, 0.3578, 0.9579], device='mps:0')\n",
      "Epoch 38/40:\n",
      "Train Loss: 0.4121, Train Acc: 0.8169\n",
      "Val Loss: 0.4280, Val Acc: 0.8037\n",
      "Current learning rate: 9.997175858951236e-05\n",
      "tensor([0.0094, 0.9708, 0.5978, 0.5187, 0.4379, 0.2250, 0.5067, 0.9756, 0.8654,\n",
      "        0.4561, 0.8158, 0.5105, 0.7165, 0.4698, 0.1170, 0.5705, 0.9343, 0.7998,\n",
      "        0.7819, 0.7439, 0.5327, 0.2896, 0.5690, 0.6371, 0.9998, 0.3084, 0.2037,\n",
      "        0.1047, 0.3841, 0.6134, 0.8305, 0.3161, 0.2128, 0.5432, 0.9999, 0.0880,\n",
      "        0.0084, 0.0536, 0.8656, 0.8211], device='mps:0')\n",
      "Epoch 39/40:\n",
      "Train Loss: 0.4062, Train Acc: 0.8137\n",
      "Val Loss: 0.4257, Val Acc: 0.8058\n",
      "Current learning rate: 9.997205909209436e-05\n",
      "tensor([0.3350, 0.9506, 0.6402, 0.4522, 0.8205, 0.4871, 0.9271, 1.0000, 0.4618,\n",
      "        0.9197, 0.8367, 0.5986, 0.2423, 0.5778, 0.9137, 0.5001, 0.6089, 0.8177,\n",
      "        0.2436, 0.9604, 0.5182, 0.7352, 0.0302, 0.9760, 0.5424, 0.2249, 0.9703,\n",
      "        0.5626, 0.8052, 0.4284, 0.9994, 0.1030, 0.2725, 0.9706, 0.7448, 1.0000,\n",
      "        0.8622, 0.9946, 0.9996, 0.7912], device='mps:0')\n",
      "Epoch 40/40:\n",
      "Train Loss: 0.4046, Train Acc: 0.8178\n",
      "Val Loss: 0.4296, Val Acc: 0.8040\n"
     ]
    }
   ],
   "source": [
    "model = CombinedDelayedRegressor().to(device)\n",
    "\n",
    "n_epochs = 40\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "model = train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, n_epochs, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nochances",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
